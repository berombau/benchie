{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"benchie","text":"<p>A tool for automating benchmarks of programming assignments.</p>"},{"location":"modules/","title":"Modules","text":""},{"location":"modules/#benchie.do_commit","title":"<code>do_commit(cwd)</code>","text":"<p>Commits the benchmark results to the repository.</p> <p>Parameters:</p> Name Type Description Default <code>cwd</code> <code>str</code> <p>The current working directory.</p> required"},{"location":"modules/#benchie.do_commit--raises","title":"Raises","text":"<pre><code>CalledProcessError: If any of the subprocess commands fail.\n</code></pre> Source code in <code>src/benchie/__init__.py</code> <pre><code>def do_commit(cwd):\n    \"\"\"\n    Commits the benchmark results to the repository.\n\n    Args:\n        cwd (str): The current working directory.\n\n    Raises\n    ------\n        CalledProcessError: If any of the subprocess commands fail.\n\n    \"\"\"\n    logger.info(\"Committing results\")\n    subprocess.run([\"git\", \"add\", \"**/*benchmark.md\"], check=True, cwd=cwd)\n    subprocess.run([\"pre-commit\", \"run\", \"--all-files\"], check=False, cwd=cwd)\n    subprocess.run([\"git\", \"add\", \"**/*benchmark.md\"], check=True, cwd=cwd)\n    subprocess.run([\"git\", \"commit\", \"-am\", \"benchmark solutions\"], check=True, cwd=cwd)\n    subprocess.run([\"git\", \"pull\", \"--rebase\"], check=True, cwd=cwd)\n    subprocess.run([\"git\", \"push\"], check=True, cwd=cwd)\n</code></pre>"},{"location":"modules/#benchie.main","title":"<code>main(output, data, force, commit, course_id, exercise_name, exercise_id, solutions, token, skip_fetch, skip_benchmark, subset, subset_data, disable_pretest, timeout, loop, loop_timeout, benchmark_options, docker_image, *args, **kwargs)</code>","text":"<p>Main function for benchmarking and processing data.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>Path to the output directory.</p> required <code>data</code> <code>str</code> <p>Path to the data folder.</p> required <code>force</code> <code>bool</code> <p>Flag indicating whether to force benchmarking even if there are no new submissions.</p> required <code>commit</code> <code>bool</code> <p>Flag indicating whether to commit the changes.</p> required <code>course_id</code> <code>str</code> <p>Course ID.</p> required <code>exercise_name</code> <code>str</code> <p>Exercise name.</p> required <code>exercise_id</code> <code>str</code> <p>Exercise ID.</p> required <code>solutions</code> <code>str</code> <p>Path to the solutions directory.</p> required <code>token</code> <code>str</code> <p>Path to the token location.</p> required <code>skip_fetch</code> <code>bool</code> <p>Flag indicating whether to skip fetching new submissions.</p> required <code>skip_benchmark</code> <code>bool</code> <p>Flag indicating whether to skip benchmarking.</p> required <code>subset</code> <code>int</code> <p>Number of solutions to consider.</p> required <code>subset_data</code> <code>int</code> <p>Number of data files to consider.</p> required <code>disable_pretest</code> <code>bool</code> <p>Flag indicating whether to test the correctness of the solutions.</p> required <code>timeout</code> <code>float</code> <p>Timeout value for benchmarking.</p> required <code>loop</code> <code>bool</code> <p>Flag indicating whether to run the benchmark in an infinite loop.</p> required <code>loop_timeout</code> <code>float</code> <p>Timeout value for the loop.</p> required <code>benchmark_options</code> <code>list[BenchmarkOption]</code> <p>List of benchmarking options.</p> required <code>docker_image</code> <code>str</code> <p>Docker image to use for benchmarking.</p> required"},{"location":"modules/#benchie.main--returns","title":"Returns","text":"<pre><code>None\n</code></pre> Source code in <code>src/benchie/__init__.py</code> <pre><code>def main(\n    output,\n    data,\n    force,\n    commit,\n    course_id,\n    exercise_name,\n    exercise_id,\n    solutions,\n    token,\n    skip_fetch,\n    skip_benchmark,\n    subset,\n    subset_data,\n    disable_pretest,\n    timeout,\n    loop,\n    loop_timeout,\n    benchmark_options,\n    docker_image,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Main function for benchmarking and processing data.\n\n    Args:\n        output (str): Path to the output directory.\n        data (str): Path to the data folder.\n        force (bool): Flag indicating whether to force benchmarking even if there are no new submissions.\n        commit (bool): Flag indicating whether to commit the changes.\n        course_id (str): Course ID.\n        exercise_name (str): Exercise name.\n        exercise_id (str): Exercise ID.\n        solutions (str): Path to the solutions directory.\n        token (str): Path to the token location.\n        skip_fetch (bool): Flag indicating whether to skip fetching new submissions.\n        skip_benchmark (bool): Flag indicating whether to skip benchmarking.\n        subset (int): Number of solutions to consider.\n        subset_data (int): Number of data files to consider.\n        disable_pretest (bool): Flag indicating whether to test the correctness of the solutions.\n        timeout (float): Timeout value for benchmarking.\n        loop (bool): Flag indicating whether to run the benchmark in an infinite loop.\n        loop_timeout (float): Timeout value for the loop.\n        benchmark_options (list[BenchmarkOption]): List of benchmarking options.\n        docker_image (str): Docker image to use for benchmarking.\n\n    Returns\n    -------\n        None\n    \"\"\"\n    cwd: Path = Path.cwd().resolve()\n    data = Path(data).resolve() / exercise_name\n    if not data.exists():\n        logger.error(f\"Data folder {data} does not exist\")\n        return\n    output = Path(output).resolve() / exercise_name\n    output.mkdir(exist_ok=True, parents=True)\n    solutions_path = Path(solutions).resolve() / exercise_name\n    while True:\n        if not skip_fetch:\n            refreshed = refresh(\n                # course id\n                course_id,\n                # exercise id\n                exercise_id,\n                # output directory\n                solutions_path,\n                # token location\n                token,\n            )\n        else:\n            refreshed = True\n        logger.info(force)\n        if force or refreshed:\n            logger.info(\"New submissions or forced\")\n            # there is new data\n            # benchmark(6, Path(\"reconstruction/J02459.1.6mers\"), output=output)\n            # benchmark(50, Path(\"reconstruction/J02459.1.50mers\"), output=output)\n            # make sure solutions are importable\n            logger.debug(solutions_path)\n            sys.path.append(str(solutions_path))\n\n            # find folders or .py files\n            all_solutions = [\n                p for p in solutions_path.iterdir() if (p.is_dir() and p.name != \"__pycache__\") or p.suffix == \".py\"\n            ][:subset]\n            logger.info(f\"Found {len(all_solutions)} solutions.\")\n            valid_solutions = all_solutions\n\n            data_paths: list[Path] = sorted(data.resolve().glob(\"data_*.py\"))[:subset_data]\n            if not data_paths:\n                logger.info(\"No data to process\")\n                return\n            for path in data_paths:\n                if not valid_solutions:\n                    logger.error(\"No valid solutions to benchmark.\")\n                    break\n                assert path.exists(), f\"Path {path} does not exist\"\n                logger.info(f\"Testing on data {path.name}\")\n                output_folder_data = output / path.stem\n                if not skip_benchmark:\n                    valid_solutions = benchmark(\n                        path,\n                        subset=subset,\n                        output=output_folder_data,\n                        solutions=valid_solutions,\n                        timeout=timeout,\n                        disable_pretest=disable_pretest,\n                        benchmark_options=benchmark_options,\n                    )\n                logger.info(\"Postprocess\")\n                postprocess_output(path, output_folder_data)\n                if commit:\n                    logger.info(\"Committing\")\n                    do_commit(cwd)\n                else:\n                    logger.info(\"Not committing\")\n        else:\n            # no new data\n            logger.info(\"No new submissions. Not Benchmarking\")\n            if loop:\n                logger.info(f\"Sleeping for {loop_timeout} seconds.\")\n                time.sleep(loop_timeout)\n        if not loop:\n            break\n</code></pre>"},{"location":"modules/#benchie.benchmark","title":"<code>benchmark</code>","text":""},{"location":"modules/#benchie.benchmark.BenchmarkOption","title":"<code>BenchmarkOption</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Benchmarking options.</p> Source code in <code>src/benchie/benchmark.py</code> <pre><code>class BenchmarkOption(Enum):\n    \"\"\"Benchmarking options.\"\"\"\n\n    HYPERFINE = \"hyperfine\"\n    SCALENE = \"scalene\"\n    MEMRAY_TRACKER = \"memray_tracker\"\n    MEMRAY_IMPORTS = \"memray_imports\"\n</code></pre>"},{"location":"modules/#benchie.benchmark.benchmark","title":"<code>benchmark(testfile, output, solutions, timeout, disable_pretest, benchmark_options, subset=None, docker_image=None)</code>","text":"<p>Perform benchmarking on submissions.</p> <p>Parameters:</p> Name Type Description Default <code>testfile</code> <code>Path</code> <p>Path to the test file.</p> required <code>subset</code> <code>str</code> <p>Subset of the test file to use.</p> <code>None</code> <code>output</code> <code>Path</code> <p>Path to the output directory.</p> required <code>solutions</code> <code>List[Path]</code> <p>List of paths to the solutions.</p> required <code>timeout</code> <code>int</code> <p>Timeout value in seconds.</p> required"},{"location":"modules/#benchie.benchmark.benchmark--returns","title":"Returns","text":"<pre><code>List[Path]: List of correct solutions.\n</code></pre> Source code in <code>src/benchie/benchmark.py</code> <pre><code>def benchmark(\n    testfile,\n    output,\n    solutions,\n    timeout,\n    disable_pretest,\n    benchmark_options: list[BenchmarkOption],\n    subset=None,\n    docker_image=None,\n):\n    \"\"\"\n    Perform benchmarking on submissions.\n\n    Args:\n        testfile (Path): Path to the test file.\n        subset (str): Subset of the test file to use.\n        output (Path): Path to the output directory.\n        solutions (List[Path]): List of paths to the solutions.\n        timeout (int): Timeout value in seconds.\n\n    Returns\n    -------\n        List[Path]: List of correct solutions.\n\n    \"\"\"\n    logger.info(\"Benchmarking submissions\")\n    if output.exists():\n        # remove everything in the output directory, except the .md files\n        for path in output.glob(\"*\"):\n            if path.suffix != \".md\":\n                if path.is_dir():\n                    shutil.rmtree(path)\n                else:\n                    path.unlink()\n    output.mkdir(exist_ok=True)\n\n    testfile = testfile.resolve()\n    prep_workdir(testfile.parent)\n\n    if not disable_pretest:\n        # test solution correctness and report errors\n        logger.info(\"Testing correctness.\")\n        all_correct_solutions = []\n        for solution in solutions:\n            try:\n                if docker_image:\n                    run_once_docker(docker_image, solution, testfile, timeout)\n                else:\n                    run_once(solution, testfile, timeout)\n                # code = with_timeout(timeout, action='timeout')(exec)(command)\n                # if code == 'timeout':\n                #     logger.error(f\"Timeout while testing '{solution.stem}'\")\n                #     continue\n                # exec(command)\n            except FileNotFoundError:\n                logger.error(f\"File not found while testing '{solution.stem}'\")\n                continue\n            except subprocess.TimeoutExpired:\n                logger.error(f\"Timeout while testing '{solution.stem}'\")\n                continue\n            except subprocess.CalledProcessError as e:\n                logger.error(f\"Error while testing '{solution.stem}'; {e}\")\n                continue\n            all_correct_solutions.append(solution)\n        logger.info(f\"Correct solutions: {len(all_correct_solutions)}\")\n    else:\n        all_correct_solutions = solutions\n\n    if BenchmarkOption.HYPERFINE.value in benchmark_options:\n        run_hyperfine_all(output, all_correct_solutions, testfile, subset=subset)\n\n    # prepare for memory profiling\n    n_memory_profiles = 3\n\n    for path in all_correct_solutions:\n        # change work dir to the solutions path\n        if BenchmarkOption.MEMRAY_TRACKER.value in benchmark_options:\n            peaks = []\n            for i in range(n_memory_profiles):\n                logger.debug(f\"Running memray on {path}, {i}\")\n                workdir = prep_workdir(testfile.parent)\n                i_output = output / f\"memray_{i}\"\n                i_output.mkdir(exist_ok=True)\n                memray_peak = run_memray(i_output, path, testfile, workdir, use_tracker=True, timeout=timeout)\n                logger.debug(f\"Peak memory usage: {memray_peak}\")\n                peaks.append(memray_peak)\n            # get median peak memory usage, with support for KiB and MiB\n            median_peak = sorted(peaks, key=lambda x: key_by_memory(x))[len(peaks) // 2]\n            logger.info(f\"Median peak memory usage: {median_peak}\")\n            # write median peak memory usage to file\n            output_peak = output / f\"{path.stem}_memray.txt\"\n            output_peak.write_text(str(median_peak))\n        if BenchmarkOption.MEMRAY_IMPORTS.value in benchmark_options:\n            logger.debug(f\"Running memray on {path}\")\n            workdir = prep_workdir(testfile.parent)\n            i_output = output / \"memray_imports\"\n            i_output.mkdir(exist_ok=True)\n            memray_peak = run_memray(i_output, path, testfile, workdir, use_tracker=False, timeout=timeout)\n            logger.debug(f\"Peak memory usage: {memray_peak}\")\n            # write median peak memory usage to file\n            output_peak = output / f\"{path.stem}_memray_imports.txt\"\n            output_peak.write_text(str(memray_peak))\n        if BenchmarkOption.SCALENE.value in benchmark_options:\n            from benchie.scalene import run_scalene\n\n            peaks = []\n            for i in range(n_memory_profiles):\n                i_output = output / f\"memory_{i}\"\n                i_output.mkdir(exist_ok=True)\n                run_scalene(i_output, path, testfile)\n    return all_correct_solutions\n</code></pre>"},{"location":"modules/#benchie.benchmark.create_command","title":"<code>create_command(path, testfile, interpreter='python')</code>","text":"<p>Create a command to execute a test file using a given path and interpreter.</p> Source code in <code>src/benchie/benchmark.py</code> <pre><code>def create_command(path, testfile, interpreter=\"python\"):\n    \"\"\"Create a command to execute a test file using a given path and interpreter.\"\"\"\n    if path.is_dir():\n        assert (path / \"src\").exists(), f\"Source folder {path / 'src'} does not exist\"\n        module = list((path / \"src\").iterdir())[0].name\n    else:\n        module = path.name.removesuffix(\".py\")\n    fn_command = testfile.read_text()\n    command = f\"\"\"import {module}; {module}.{fn_command}\n    \"\"\"\n    return command\n</code></pre>"},{"location":"modules/#benchie.reporting","title":"<code>reporting</code>","text":""},{"location":"modules/#benchie.reporting.create_table","title":"<code>create_table(with_imports, with_tracker, timings=None)</code>","text":"<p>Expected output:</p> Command Mean [s] Min [s] Max [s] Rank <code>13309298</code> 4.500 \u00b1 0.036 4.474 4.541 1.00 <code>13309297</code> 4.515 \u00b1 0.116 4.445 4.648 1.00 \u00b1 0.03 <p>create_table({'13309298': '1.0 MB'}) ' | Command | Peak memory | Rank | \\n | :--- | ---: | ---: | \\n | <code>13309298</code> |  |  |  | 1.0 MB | 0 | ' create_table({'13309298': '1.0 MB'}, {'results': [{'command': '13309298', 'mean': 4.5, 'stddev': 0.036, 'min': 4.474, 'max': 4.541}]}) ' | Command | Mean [s] | Min [s] | Max [s] | Peak memory | Rank | \\n | :--- | ---: | ---: | ---: | ---: | ---: | \\n | <code>13309298</code> | 4.500 \u00b1 0.036 | 4.474 | 4.541 | 1.0 MB | 0 | '</p> Source code in <code>src/benchie/reporting.py</code> <pre><code>def create_table(with_imports, with_tracker, timings=None):\n    r\"\"\"\n    Expected output:\n\n    | Command | Mean [s] | Min [s] | Max [s] | Rank |\n    |:---|---:|---:|---:|---:|\n    | `13309298` | 4.500 \u00b1 0.036 | 4.474 | 4.541 | 1.00 |\n    | `13309297` | 4.515 \u00b1 0.116 | 4.445 | 4.648 | 1.00 \u00b1 0.03 |\n\n    &gt;&gt;&gt; create_table({'13309298': '1.0 MB'})\n    ' | Command | Peak memory | Rank | \\n | :--- | ---: | ---: | \\n | `13309298` |  |  |  | 1.0 MB | 0 | '\n    &gt;&gt;&gt; create_table({'13309298': '1.0 MB'}, {'results': [{'command': '13309298', 'mean': 4.5, 'stddev': 0.036, 'min': 4.474, 'max': 4.541}]})\n    ' | Command | Mean [s] | Min [s] | Max [s] | Peak memory | Rank | \\n | :--- | ---: | ---: | ---: | ---: | ---: | \\n | `13309298` | 4.500 \u00b1 0.036 | 4.474 | 4.541 | 1.0 MB | 0 | '\n    \"\"\"\n    output = []\n    d = \" | \"\n\n    if timings is None:\n        header = [\"Command\"]\n        if with_imports:\n            header.append(\"(with_imports) Peak memory\")\n        if with_tracker:\n            header.append(\"(with_tracker) Median peak memory\")\n        header.append(\"Rank\")\n        output.append(d + d.join(header) + d)\n        output.append(d + d.join([\":---\", *[\"---:\" for _ in range(len(header) - 1)]]) + d)\n\n        relative_peaks = make_relative(with_imports or with_tracker)\n        for k in relative_peaks:\n            columns = [f\"`{k}`\"]\n            if with_imports:\n                columns.append(with_imports[k])\n            if with_tracker:\n                columns.append(with_tracker[k])\n            columns.append(str(relative_peaks[k]))\n            output.append(d + d.join([str(c) for c in columns]) + d)\n    else:\n        header = [\"Command\", \"Mean [s]\", \"Min [s]\", \"Max [s]\"]\n        if with_imports:\n            header.append(\"(with_imports) Peak memory\")\n        if with_tracker:\n            header.append(\"(with_tracker) Median peak memory\")\n        header.append(\"Rank\")\n        output.append(d + d.join(header) + d)\n        output.append(d + d.join([\":---\", *[\"---:\" for _ in range(len(header) - 1)]]) + d)\n        d_relative = make_relative(with_imports or with_tracker)\n        for c in timings[\"results\"]:\n            name = c[\"command\"]\n            columns = [\n                f\"{x:.3f}\" if isinstance(x, float) else str(x)\n                for x in [\n                    f\"`{name}`\",\n                    # mean + stdev,\n                    f\"{c['mean']:.3f} \u00b1 {c['stddev']:.3f}\",\n                    # min,\n                    c[\"min\"],\n                    # max,\n                    c[\"max\"],\n                ]\n            ]\n            if with_imports:\n                columns.append(with_imports[name])\n            if with_tracker:\n                columns.append(with_tracker[name])\n            columns.append(str(d_relative[name]))\n            output.append(d + d.join(columns) + d)\n    return \"\\n\".join(output)\n</code></pre>"},{"location":"modules/#benchie.reporting.key_by_memory","title":"<code>key_by_memory(s)</code>","text":"<p>summary</p> <p>:param s: description :return: description</p> <p>key_by_memory('1.0 MB') 1.0 key_by_memory('1.0 GB') 1000.0</p> Source code in <code>src/benchie/reporting.py</code> <pre><code>def key_by_memory(s):\n    \"\"\"_summary_\n\n    :param s: _description_\n    :return: _description_\n    &gt;&gt;&gt; key_by_memory('1.0 MB')\n    1.0\n    &gt;&gt;&gt; key_by_memory('1.0 GB')\n    1000.0\n    \"\"\"\n    if s.endswith(\"KB\") or s.endswith(\"KiB\"):\n        return float(s[:-3]) / 1000\n    elif s.endswith(\"MB\") or s.endswith(\"MiB\"):\n        return float(s[:-3])\n    elif s.endswith(\"GB\") or s.endswith(\"GiB\"):\n        return float(s[:-3]) * 1000\n    else:\n        # e.g. None\n        return float(\"inf\")\n</code></pre>"},{"location":"modules/#benchie.reporting.make_relative","title":"<code>make_relative(d)</code>","text":"<p>summary</p> <p>:param d: description :return: description</p> <p>make_relative({'a': '1.0 MB', 'b': '2.0 MB'}) {'a': 0, 'b': 1} make_relative({'a': '1.0 MB', 'b': '1.0 MB'}) {'a': 0, 'b': 1} make_relative({'a': '1.0 GB', 'b': '2.0 KB'}) {'b': 0, 'a': 1}</p> Source code in <code>src/benchie/reporting.py</code> <pre><code>def make_relative(d):\n    \"\"\"_summary_\n\n    :param d: _description_\n    :return: _description_\n    &gt;&gt;&gt; make_relative({'a': '1.0 MB', 'b': '2.0 MB'})\n    {'a': 0, 'b': 1}\n    &gt;&gt;&gt; make_relative({'a': '1.0 MB', 'b': '1.0 MB'})\n    {'a': 0, 'b': 1}\n    &gt;&gt;&gt; make_relative({'a': '1.0 GB', 'b': '2.0 KB'})\n    {'b': 0, 'a': 1}\n    \"\"\"\n    sort_d = sorted(d.items(), key=lambda x: key_by_memory(x[1]))\n    d_relative = {k: i for i, (k, _) in enumerate(sort_d)}\n    return d_relative\n</code></pre>"}]}